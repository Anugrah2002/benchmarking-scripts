name: BenchmarkDoublemodelRating

on:
  workflow_dispatch: # This allows the workflow to be triggered manually

permissions:
  contents: read

jobs:
  Phi-3-miniQ4:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: 'pip'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai
        wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
        wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf
        mv Phi-3-mini-4k-instruct-q4.gguf /tmp/llama-model.gguf
        mv Meta-Llama-3-8B-Instruct.Q8_0.gguf /tmp/llamarater.gguf
        mv outputformat.json /tmp/outputformat.json
        docker run --name main -d  -p 8000:8000 -v /tmp:/models -e MODEL=/models/llama-model.gguf ghcr.io/abetlen/llama-cpp-python:v0.2.79
        docker run --name rater -d -p 8080:8080 -v /tmp:/models ghcr.io/ggerganov/llama.cpp:server -m /models/llamarater.gguf --host 0.0.0.0 --port 8080 --json-schema /models/outputformat.json
    - name: Running the script
      run: |
        python benchmarkopenaiwithrater.py
    - name: Trail Docker Logs
      run: |
        docker logs main
        echo "==========END OF MAIN CONTAINER ==========="
        docker logs rater

  Phi-3-minifp16:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: 'pip'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai
        wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf
        wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf
        mv outputformat.json /tmp/outputformat.json
        mv Phi-3-mini-4k-instruct-fp16.gguf /tmp/llama-model.gguf
        mv Meta-Llama-3-8B-Instruct.Q8_0.gguf /tmp/llamarater.gguf
        docker run --name main -d  -p 8000:8000 -v /tmp:/models -e MODEL=/models/llama-model.gguf ghcr.io/abetlen/llama-cpp-python:v0.2.79
        docker run --name rater -d -p 8080:8080 -v /tmp:/models ghcr.io/ggerganov/llama.cpp:server -m /models/llamarater.gguf --host 0.0.0.0 --port 8080 --json-schema /models/outputformat.json

    - name: Running the script
      run: |
        python benchmarkopenaiwithrater.py
    - name: Trail Docker Logs
      run: |
        docker logs main
        echo "==========END OF MAIN CONTAINER ==========="
        docker logs rater

  Meta-Llama-3-8B-InstructQ2_K:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: 'pip'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai
        wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q2_K.gguf
        wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf
        mv Meta-Llama-3-8B-Instruct.Q2_K.gguf /tmp/llama-model.gguf
        mv Meta-Llama-3-8B-Instruct.Q8_0.gguf /tmp/llamarater.gguf
        mv outputformat.json /tmp/outputformat.json
        docker run --name main -d  -p 8000:8000 -v /tmp:/models -e MODEL=/models/llama-model.gguf ghcr.io/abetlen/llama-cpp-python:v0.2.79
        docker run --name rater -d -p 8080:8080 -v /tmp:/models ghcr.io/ggerganov/llama.cpp:server -m /models/llamarater.gguf --host 0.0.0.0 --port 8080 --json-schema /models/outputformat.json

    - name: Running the script
      run: |
        python benchmarkopenaiwithrater.py
    - name: Trail Docker Logs
      run: |
        docker logs main
        echo "==========END OF MAIN CONTAINER ==========="
        docker logs rater

  Meta-Llama-3-8B-InstructQ4_K_M:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: 'pip'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai
        wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf
        wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf
        mv Meta-Llama-3-8B-Instruct.Q4_K_M.gguf /tmp/llama-model.gguf
        mv Meta-Llama-3-8B-Instruct.Q8_0.gguf /tmp/llamarater.gguf
        mv outputformat.json /tmp/outputformat.json
        docker run --name main -d  -p 8000:8000 -v /tmp:/models -e MODEL=/models/llama-model.gguf ghcr.io/abetlen/llama-cpp-python:v0.2.79
        docker run --name rater -d -p 8080:8080 -v /tmp:/models ghcr.io/ggerganov/llama.cpp:server -m /models/llamarater.gguf --host 0.0.0.0 --port 8080 --json-schema /models/outputformat.json

    - name: Running the script
      run: |
        python benchmarkopenaiwithrater.py
    - name: Trail Docker Logs
      run: |
        docker logs main
        echo "==========END OF MAIN CONTAINER ==========="
        docker logs rater
        
  Meta-Llama-3-8B-InstructQ8_0:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
        cache: 'pip'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install openai
        wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf
        mv Meta-Llama-3-8B-Instruct.Q8_0.gguf /tmp/llama-model.gguf
        wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf
        mv Meta-Llama-3-8B-Instruct.Q8_0.gguf /tmp/llamarater.gguf
        mv outputformat.json /tmp/outputformat.json
        docker run --name main -d  -p 8000:8000 -v /tmp:/models -e MODEL=/models/llama-model.gguf ghcr.io/abetlen/llama-cpp-python:v0.2.79
        docker run --name rater -d -p 8080:8080 -v /tmp:/models ghcr.io/ggerganov/llama.cpp:server -m /models/llamarater.gguf --host 0.0.0.0 --port 8080 --json-schema /models/outputformat.json

    - name: Running the script
      run: |
        python benchmarkopenaiwithrater.py
    - name: Trail Docker Logs
      run: |
        docker logs main
        echo "==========END OF MAIN CONTAINER ==========="
        docker logs rater
